"""
CyberX Malware Analysis Service
Production-ready malware detection combining signature-based, heuristic, and ML methods.
"""

import os
import re
import math
import json
import hashlib
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from collections import Counter
from pathlib import Path

import numpy as np
from django.shortcuts import render
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods
from django.core.files.uploadedfile import UploadedFile
import joblib

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('MalwareAnalysis')

# Try to import pefile
try:
    import pefile
    PEFILE_AVAILABLE = True
except ImportError:
    PEFILE_AVAILABLE = False
    logger.warning("pefile not available - PE analysis will be limited")


# =============================================================================
# Signature Database
# =============================================================================

class SignatureDatabase:
    """Known malware signatures and indicators"""
    
    KNOWN_MALWARE_HASHES = {
        "ed01ebfbc9eb5bbea545af4d01bf5f1071661840480439c6e5babe8e080e41aa": "WannaCry",
        "24d004a104d4d54034dbcffc2a4b19a11f39008a575aa614ea04703480b1022c": "WannaCry",
        "027cc450ef5f8c5f653329641ec1fed91f694e0d229928963b30f6b0d7d3a745": "Petya",
    }
    
    SUSPICIOUS_STRINGS = [
        b"Your files have been encrypted", b"bitcoin", b"ransom", b"decrypt", b".onion",
        b"GetAsyncKeyState", b"keylog", b"HKEY_LOCAL_MACHINE", b"CurrentVersion\\Run",
        b"cmd.exe /c", b"powershell -enc", b"IEX(", b"Invoke-Expression", b"DownloadString",
        b"Net.WebClient", b"VirtualAlloc", b"WriteProcessMemory", b"CreateRemoteThread",
        b"NtUnmapViewOfSection", b"IsDebuggerPresent", b"vmware", b"virtualbox", b"sandbox",
    ]
    
    SUSPICIOUS_IMPORTS = {
        'high_risk': [
            'CreateRemoteThread', 'WriteProcessMemory', 'VirtualAllocEx',
            'NtUnmapViewOfSection', 'QueueUserAPC', 'SetWindowsHookEx',
            'GetAsyncKeyState', 'RtlCreateUserThread',
        ],
        'medium_risk': [
            'VirtualAlloc', 'VirtualProtect', 'LoadLibrary', 'GetProcAddress',
            'CreateProcess', 'ShellExecute', 'URLDownloadToFile', 'InternetOpen',
        ],
    }
    
    SUSPICIOUS_SECTIONS = [
        '.UPX0', '.UPX1', '.aspack', '.nsp0', '.pec', '.perplex', '.yP', '.packed',
    ]


# =============================================================================
# Feature Extractor
# =============================================================================

class FeatureExtractor:
    """Extract features from files for malware analysis"""
    
    def calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy"""
        if not data:
            return 0.0
        byte_counts = Counter(data)
        total_bytes = len(data)
        entropy = 0.0
        for count in byte_counts.values():
            if count > 0:
                probability = count / total_bytes
                entropy -= probability * math.log2(probability)
        return entropy
    
    def extract_strings(self, data: bytes, min_length: int = 4) -> Dict[str, Any]:
        """Extract strings from binary data"""
        ascii_pattern = rb'[\x20-\x7e]{%d,}' % min_length
        ascii_strings = re.findall(ascii_pattern, data)
        
        unicode_pattern = rb'(?:[\x20-\x7e]\x00){%d,}' % min_length
        unicode_matches = re.findall(unicode_pattern, data)
        unicode_strings = [m.replace(b'\x00', b'') for m in unicode_matches]
        
        all_strings = ascii_strings + unicode_strings
        urls = [s for s in all_strings if b'http' in s.lower()]
        ips = re.findall(rb'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', data)
        registry = [s for s in all_strings if b'HKEY_' in s.upper()]
        
        return {
            'total_strings': len(all_strings),
            'ascii_strings': len(ascii_strings),
            'unicode_strings': len(unicode_strings),
            'url_count': len(urls),
            'ip_count': len(ips),
            'registry_count': len(registry),
            'avg_string_length': np.mean([len(s) for s in all_strings]) if all_strings else 0,
            'max_string_length': max([len(s) for s in all_strings]) if all_strings else 0,
            'strings': all_strings[:100],
            'urls': urls,
            'ips': ips,
        }
    
    def analyze_pe(self, data: bytes) -> Dict[str, Any]:
        """Analyze PE file structure"""
        pe_features = {
            'is_pe': False, 'is_dll': False, 'is_exe': False, 'num_sections': 0,
            'num_imports': 0, 'suspicious_sections': 0, 'suspicious_imports_high': 0,
            'suspicious_imports_medium': 0, 'has_signature': False,
            'section_entropy_avg': 0, 'section_entropy_max': 0, 'virtual_size_ratio': 0,
        }
        
        if data[:2] != b'MZ':
            return pe_features
        
        pe_features['is_pe'] = True
        
        if not PEFILE_AVAILABLE:
            return pe_features
        
        try:
            pe = pefile.PE(data=data, fast_load=True)
            
            if pe.FILE_HEADER.Characteristics & 0x2000:
                pe_features['is_dll'] = True
            elif pe.FILE_HEADER.Characteristics & 0x0002:
                pe_features['is_exe'] = True
            
            pe_features['num_sections'] = pe.FILE_HEADER.NumberOfSections
            
            section_entropies = []
            total_virtual = total_raw = 0
            
            for section in pe.sections:
                section_name = section.Name.decode('utf-8', errors='ignore').strip('\x00')
                section_entropies.append(section.get_entropy())
                total_virtual += section.Misc_VirtualSize
                total_raw += section.SizeOfRawData
                
                if section_name.upper() in [s.upper() for s in SignatureDatabase.SUSPICIOUS_SECTIONS]:
                    pe_features['suspicious_sections'] += 1
            
            if section_entropies:
                pe_features['section_entropy_avg'] = np.mean(section_entropies)
                pe_features['section_entropy_max'] = max(section_entropies)
            
            if total_raw > 0:
                pe_features['virtual_size_ratio'] = total_virtual / total_raw
            
            pe.parse_data_directories()
            
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                pe_features['num_imports'] = len(pe.DIRECTORY_ENTRY_IMPORT)
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    for imp in entry.imports:
                        if imp.name:
                            import_name = imp.name.decode('utf-8', errors='ignore')
                            if import_name in SignatureDatabase.SUSPICIOUS_IMPORTS['high_risk']:
                                pe_features['suspicious_imports_high'] += 1
                            elif import_name in SignatureDatabase.SUSPICIOUS_IMPORTS['medium_risk']:
                                pe_features['suspicious_imports_medium'] += 1
            
            pe_features['has_signature'] = hasattr(pe, 'DIRECTORY_ENTRY_SECURITY')
            pe.close()
            
        except Exception as e:
            logger.debug(f"PE analysis error: {e}")
        
        return pe_features
    
    def extract_all_features(self, data: bytes, filename: str = "") -> Dict[str, Any]:
        """Extract all features from a file"""
        features = {
            'file_size': len(data),
            'file_size_log': math.log10(len(data) + 1),
            'md5': hashlib.md5(data).hexdigest(),
            'sha1': hashlib.sha1(data).hexdigest(),
            'sha256': hashlib.sha256(data).hexdigest(),
        }
        
        # File type detection
        ext = os.path.splitext(filename)[1].lower() if filename else ''
        features['is_executable'] = int(data[:2] == b'MZ' or ext in ['.exe', '.dll', '.sys'])
        features['is_script'] = int(ext in ['.ps1', '.bat', '.vbs', '.js', '.cmd'])
        features['is_document'] = int(ext in ['.doc', '.docx', '.xls', '.xlsx', '.pdf'])
        features['is_archive'] = int(ext in ['.zip', '.rar', '.7z'])
        
        # Entropy
        features['overall_entropy'] = self.calculate_entropy(data)
        features['header_entropy'] = self.calculate_entropy(data[:256])
        features['middle_entropy'] = self.calculate_entropy(data[len(data)//2:len(data)//2+256])
        features['footer_entropy'] = self.calculate_entropy(data[-256:])
        features['is_high_entropy'] = int(features['overall_entropy'] > 7.0)
        features['is_packed'] = int(features['overall_entropy'] > 7.5)
        
        # Strings
        string_analysis = self.extract_strings(data)
        features.update({k: v for k, v in string_analysis.items() if k not in ['strings', 'urls', 'ips']})
        features['path_count'] = len([s for s in string_analysis['strings'] if b':\\' in s])
        features['_strings'] = string_analysis['strings']
        features['_urls'] = string_analysis['urls']
        features['_ips'] = string_analysis['ips']
        
        # Suspicious patterns
        suspicious_count = sum(1 for p in SignatureDatabase.SUSPICIOUS_STRINGS if p.lower() in data.lower())
        features['suspicious_string_count'] = suspicious_count
        
        # PE analysis
        pe_features = self.analyze_pe(data)
        features.update(pe_features)
        
        # Add missing features for ML model compatibility
        features['is_driver'] = 0
        features['num_exports'] = 0
        features['has_debug_info'] = 0
        features['has_tls'] = 0
        features['has_resources'] = 0
        features['has_relocations'] = 0
        features['suspicious_imports_low'] = 0
        
        return features


# =============================================================================
# Heuristic Analyzer
# =============================================================================

class HeuristicAnalyzer:
    """Rule-based heuristic analysis"""
    
    def __init__(self):
        self.rules = [
            {'id': 'HEUR001', 'name': 'High Entropy Executable', 'severity': 'high', 'score': 30,
             'check': lambda f: f.get('is_pe') and f.get('overall_entropy', 0) > 7.0},
            {'id': 'HEUR002', 'name': 'Suspicious API Imports', 'severity': 'high', 'score': 40,
             'check': lambda f: f.get('suspicious_imports_high', 0) >= 2},
            {'id': 'HEUR003', 'name': 'Process Injection Capability', 'severity': 'critical', 'score': 50,
             'check': lambda f: f.get('suspicious_imports_high', 0) >= 3},
            {'id': 'HEUR004', 'name': 'Packed Executable', 'severity': 'medium', 'score': 25,
             'check': lambda f: f.get('suspicious_sections', 0) > 0 or f.get('is_packed')},
            {'id': 'HEUR005', 'name': 'Network Indicators', 'severity': 'low', 'score': 10,
             'check': lambda f: f.get('url_count', 0) > 0 or f.get('ip_count', 0) > 0},
            {'id': 'HEUR006', 'name': 'Registry Modification', 'severity': 'medium', 'score': 20,
             'check': lambda f: f.get('registry_count', 0) > 0},
            {'id': 'HEUR007', 'name': 'Suspicious Strings', 'severity': 'high', 'score': 35,
             'check': lambda f: f.get('suspicious_string_count', 0) >= 3},
            {'id': 'HEUR008', 'name': 'No Digital Signature', 'severity': 'low', 'score': 10,
             'check': lambda f: f.get('is_pe') and not f.get('has_signature')},
            {'id': 'HEUR009', 'name': 'PowerShell Encoded', 'severity': 'critical', 'score': 50,
             'check': lambda f: any(b'-enc ' in s or b'FromBase64String' in s for s in f.get('_strings', []))},
            {'id': 'HEUR010', 'name': 'Download Capability', 'severity': 'high', 'score': 30,
             'check': lambda f: any(b'DownloadString' in s or b'URLDownloadToFile' in s for s in f.get('_strings', []))},
        ]
    
    def analyze(self, features: Dict) -> Dict[str, Any]:
        """Run heuristic analysis"""
        triggered = []
        total_score = 0
        severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        
        for rule in self.rules:
            try:
                if rule['check'](features):
                    triggered.append({
                        'id': rule['id'],
                        'name': rule['name'],
                        'severity': rule['severity'],
                        'score': rule['score']
                    })
                    total_score += rule['score']
                    severity_counts[rule['severity']] += 1
            except:
                pass
        
        if total_score >= 80 or severity_counts['critical'] > 0:
            threat_level = 'critical'
        elif total_score >= 50 or severity_counts['high'] >= 2:
            threat_level = 'high'
        elif total_score >= 25:
            threat_level = 'medium'
        elif total_score > 0:
            threat_level = 'low'
        else:
            threat_level = 'clean'
        
        return {
            'heuristic_score': total_score,
            'threat_level': threat_level,
            'triggered_rules': triggered,
            'rule_count': len(triggered),
            'severity_counts': severity_counts
        }


# =============================================================================
# Main Analyzer
# =============================================================================

class MalwareAnalyzer:
    """Production malware analyzer"""
    
    def __init__(self):
        self.feature_extractor = FeatureExtractor()
        self.heuristic_analyzer = HeuristicAnalyzer()
        self.rf_model = None
        self.gb_model = None
        self.scaler = None
        self.feature_names = None
        self._load_models()
    
    def _load_models(self):
        """Load ML models"""
        base_path = Path(__file__).parent.parent.parent / 'Services' / 'MalwareAnalysis' / 'models'
        
        try:
            self.rf_model = joblib.load(base_path / 'malware_rf_model.joblib')
            self.gb_model = joblib.load(base_path / 'malware_gb_model.joblib')
            self.scaler = joblib.load(base_path / 'malware_scaler.joblib')
            with open(base_path / 'malware_feature_names.json', 'r') as f:
                self.feature_names = json.load(f)
            logger.info("âœ… ML models loaded successfully")
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load ML models: {e}")
    
    def analyze_file(self, file_data: bytes, filename: str = "unknown") -> Dict[str, Any]:
        """Analyze a file for malware"""
        start_time = datetime.now()
        
        result = {
            'filename': filename,
            'file_size': len(file_data),
            'file_size_formatted': self._format_size(len(file_data)),
            'analysis_time': None,
            'verdict': 'unknown',
            'threat_score': 0,
            'confidence': 0.0,
            'is_malicious': False,
            'detection_methods': [],
            'signatures': {},
            'heuristic_analysis': {},
            'ml_analysis': {},
            'features': {},
            'recommendations': [],
            'risk_factors': [],
        }
        
        try:
            # Extract features
            features = self.feature_extractor.extract_all_features(file_data, filename)
            
            result['signatures'] = {
                'md5': features['md5'],
                'sha1': features['sha1'],
                'sha256': features['sha256']
            }
            
            # Signature check
            malware_name = SignatureDatabase.KNOWN_MALWARE_HASHES.get(features['sha256'])
            if malware_name:
                result['verdict'] = 'malicious'
                result['is_malicious'] = True
                result['threat_score'] = 100
                result['confidence'] = 100.0
                result['detection_methods'].append('signature')
                result['risk_factors'].append(f'Known malware: {malware_name}')
            
            # Heuristic analysis
            heuristic_result = self.heuristic_analyzer.analyze(features)
            result['heuristic_analysis'] = heuristic_result
            
            if heuristic_result['threat_level'] in ['critical', 'high']:
                result['detection_methods'].append('heuristic')
                result['risk_factors'].extend([
                    f"{r['name']}" for r in heuristic_result['triggered_rules']
                ])
            
            # ML analysis
            if self.rf_model and self.scaler and self.feature_names:
                try:
                    ml_features = np.array([features.get(name, 0) for name in self.feature_names], dtype=np.float64)
                    ml_scaled = self.scaler.transform(ml_features.reshape(1, -1))
                    
                    rf_pred = self.rf_model.predict(ml_scaled)[0]
                    rf_proba = self.rf_model.predict_proba(ml_scaled)[0]
                    gb_pred = self.gb_model.predict(ml_scaled)[0]
                    gb_proba = self.gb_model.predict_proba(ml_scaled)[0]
                    
                    ensemble_proba = 0.6 * rf_proba[1] + 0.4 * gb_proba[1]
                    
                    result['ml_analysis'] = {
                        'rf_prediction': int(rf_pred),
                        'rf_confidence': float(rf_proba[1]) * 100,
                        'gb_prediction': int(gb_pred),
                        'gb_confidence': float(gb_proba[1]) * 100,
                        'ensemble_confidence': float(ensemble_proba) * 100,
                        'is_malicious': ensemble_proba > 0.5
                    }
                    
                    if result['ml_analysis']['is_malicious']:
                        result['detection_methods'].append('machine_learning')
                except Exception as e:
                    logger.debug(f"ML analysis error: {e}")
            
            # Calculate threat score
            score = 0
            if 'signature' not in result['detection_methods']:
                score += min(40, heuristic_result['heuristic_score'] * 0.5)
                if result['ml_analysis'].get('is_malicious'):
                    score += (result['ml_analysis'].get('ensemble_confidence', 0) / 100) * 40
                if features.get('is_packed'):
                    score += 5
                if features.get('suspicious_imports_high', 0) >= 2:
                    score += 5
                if features.get('suspicious_string_count', 0) >= 5:
                    score += 5
                if not features.get('has_signature') and features.get('is_pe'):
                    score += 5
                result['threat_score'] = min(100, int(score))
            
            # Final verdict
            if result['threat_score'] >= 80:
                result['verdict'] = 'malicious'
                result['is_malicious'] = True
            elif result['threat_score'] >= 50:
                result['verdict'] = 'suspicious'
                result['is_malicious'] = True
            elif result['threat_score'] >= 25:
                result['verdict'] = 'potentially_unwanted'
            else:
                result['verdict'] = 'clean'
            
            # Confidence
            method_count = len(result['detection_methods'])
            if method_count >= 3:
                result['confidence'] = 95.0
            elif method_count == 2:
                result['confidence'] = 85.0
            elif method_count == 1:
                result['confidence'] = 70.0
            else:
                result['confidence'] = 90.0 if result['threat_score'] < 10 else 60.0
            
            # Recommendations
            if result['verdict'] == 'malicious':
                result['recommendations'] = [
                    "ðŸš¨ DELETE this file immediately",
                    "Run a full system scan",
                    "Check for persistence mechanisms"
                ]
            elif result['verdict'] == 'suspicious':
                result['recommendations'] = [
                    "âš ï¸ Quarantine this file",
                    "Submit to VirusTotal",
                    "Do not execute"
                ]
            elif result['verdict'] == 'potentially_unwanted':
                result['recommendations'] = [
                    "Review before execution",
                    "Verify the source"
                ]
            else:
                result['recommendations'] = ["âœ… File appears safe"]
            
            # Key features for display
            result['features'] = {
                'file_type': 'PE Executable' if features.get('is_pe') else 
                            'Script' if features.get('is_script') else
                            'Document' if features.get('is_document') else 'Unknown',
                'entropy': round(features.get('overall_entropy', 0), 2),
                'is_packed': features.get('is_packed', False),
                'suspicious_strings': features.get('suspicious_string_count', 0),
                'suspicious_imports': features.get('suspicious_imports_high', 0),
                'url_count': features.get('url_count', 0),
                'ip_count': features.get('ip_count', 0),
            }
            
        except Exception as e:
            logger.error(f"Analysis error: {e}")
            result['error'] = str(e)
            result['verdict'] = 'error'
        
        result['analysis_time'] = (datetime.now() - start_time).total_seconds() * 1000
        return result
    
    def _format_size(self, size: int) -> str:
        """Format file size for display"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} TB"


# Initialize analyzer
analyzer = MalwareAnalyzer()


# =============================================================================
# Django Views
# =============================================================================

def index(request):
    """Main malware analysis page"""
    result = None
    
    if request.method == 'POST' and request.FILES.get('file'):
        uploaded_file: UploadedFile = request.FILES['file']
        
        # Check file size (max 50MB)
        if uploaded_file.size > 50 * 1024 * 1024:
            result = {
                'error': True,
                'error_message': 'File size exceeds 50MB limit',
                'filename': uploaded_file.name
            }
        else:
            file_data = uploaded_file.read()
            result = analyzer.analyze_file(file_data, uploaded_file.name)
            
            # Add status color and icon
            verdict_styles = {
                'clean': ('success', 'fa-check-circle'),
                'potentially_unwanted': ('warning', 'fa-exclamation-triangle'),
                'suspicious': ('danger', 'fa-exclamation-circle'),
                'malicious': ('danger', 'fa-skull-crossbones'),
                'error': ('secondary', 'fa-times-circle'),
            }
            style = verdict_styles.get(result['verdict'], ('secondary', 'fa-question-circle'))
            result['status_color'] = style[0]
            result['status_icon'] = f'fas {style[1]}'
    
    context = {'result': result}
    return render(request, 'MalwareAnalysis.html', context)


@csrf_exempt
@require_http_methods(["POST"])
def api_analyze(request):
    """API endpoint for malware analysis"""
    if not request.FILES.get('file'):
        return JsonResponse({'error': 'No file provided'}, status=400)
    
    uploaded_file: UploadedFile = request.FILES['file']
    
    if uploaded_file.size > 50 * 1024 * 1024:
        return JsonResponse({'error': 'File size exceeds 50MB limit'}, status=400)
    
    file_data = uploaded_file.read()
    result = analyzer.analyze_file(file_data, uploaded_file.name)
    
    return JsonResponse(result)
